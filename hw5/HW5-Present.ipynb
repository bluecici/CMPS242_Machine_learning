{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HW5 Hillary or Donald?\n",
    "\n",
    "### Jiahui Wei, Yunzheng Zhang, Yusi Xiang\n",
    "\n",
    "Classify tweets from Hillary Clinton and Donald Trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. Logistic Regression\n",
    "2. SVM\n",
    "3. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:30:03.935243Z",
     "start_time": "2017-11-21T01:30:01.613217Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import for Logistic Regression and SVM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import log_loss\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:30:05.538861Z",
     "start_time": "2017-11-21T01:30:05.507833Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_data = pd.read_csv(\"./train.csv\")\n",
    "train_data[\"label_num\"] = train_data.handle.map({\n",
    "    \"HillaryClinton\": 0,\n",
    "    \"realDonaldTrump\": 1\n",
    "})\n",
    "\n",
    "test_data = pd.read_csv('./new_test.csv')\n",
    "test_data[\"label_num\"] = test_data.handle.map({\n",
    "    \"HillaryClinton\": 0,\n",
    "    \"realDonaldTrump\": 1\n",
    "})\n",
    "\n",
    "X_train, y_train = train_data[\"tweet\"], train_data[\"label_num\"]\n",
    "\n",
    "X_test, y_test = test_data[\"tweet\"], test_data[\"label_num\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First try using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:30:10.316262Z",
     "start_time": "2017-11-21T01:30:10.150669Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(\n",
    "    decode_error='ignore', stop_words=stopwords.words(\"english\"))\n",
    "X_train_count = count_vec.fit_transform(X_train)\n",
    "X_test_count = count_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:30:11.588998Z",
     "start_time": "2017-11-21T01:30:11.582604Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_trans = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_trans.fit_transform(X_train_count)\n",
    "X_test_tfidf = tfidf_trans.transform(X_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:32:30.114660Z",
     "start_time": "2017-11-21T01:30:13.567108Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train_tfidf_rd = pca.fit_transform(X_train_tfidf.toarray())\n",
    "X_test_tfidf_rd = pca.transform(X_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:32:50.525272Z",
     "start_time": "2017-11-21T01:32:34.078551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.914756025867, log loss is 0.234298274729\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegressionCV()\n",
    "lr.fit(X_train_tfidf_rd, y_train)\n",
    "logls = (log_loss(y_test, lr.predict_proba(X_test_tfidf_rd), labels=[0, 1]))\n",
    "accuracy = sum(lr.predict(X_test_tfidf_rd) == y_test) * \\\n",
    "    1.0 / X_test_tfidf_rd.shape[0]\n",
    "\n",
    "print 'Accuracy is ' + str(accuracy) + ', log loss is ' + str(logls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use a better tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:33:23.455644Z",
     "start_time": "2017-11-21T01:33:23.443727Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'question', 'in', 'this', 'election', 'Who', 'can', 'put', 'the', 'plans', 'into', 'action', 'that', 'will', 'make', 'your', 'life', 'better', 'https', 'co', 'XreEY9OicG']\n"
     ]
    }
   ],
   "source": [
    "built_in_tokenizer = count_vec.build_tokenizer()\n",
    "tokens = built_in_tokenizer(X_train[0])\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:33:28.897037Z",
     "start_time": "2017-11-21T01:33:28.893003Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'The', u'question', u'in', u'this', u'election', u':', u'Who', u'can', u'put', u'the', u'plans', u'into', u'action', u'that', u'will', u'make', u'your', u'life', u'better', u'?', u'https://t.co/XreEY9OicG']\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "tokens = tknzr.tokenize(X_train[0])\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T00:33:43.954088Z",
     "start_time": "2017-11-21T00:33:43.951218Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:33:38.205927Z",
     "start_time": "2017-11-21T01:33:38.201572Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    tknzr = TweetTokenizer()\n",
    "    try:\n",
    "        tweet = tweet.lower()\n",
    "        tokens = tknzr.tokenize(tweet)\n",
    "        tokens = map(lambda t: t if not t.startswith(\n",
    "            'http') else '<url>', tokens)\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:33:43.053351Z",
     "start_time": "2017-11-21T01:33:42.308079Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(\n",
    "    decode_error='ignore', tokenizer=tokenize, stop_words=stopwords.words(\"english\"))\n",
    "X_train_count = count_vec.fit_transform(X_train)\n",
    "X_test_count = count_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:33:45.062416Z",
     "start_time": "2017-11-21T01:33:44.854510Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_trans = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_trans.fit_transform(X_train_count)\n",
    "X_test_tfidf = tfidf_trans.transform(X_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:35:27.738108Z",
     "start_time": "2017-11-21T01:33:46.624279Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train_tfidf_rd = pca.fit_transform(X_train_tfidf.toarray())\n",
    "X_test_tfidf_rd = pca.transform(X_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:35:53.051845Z",
     "start_time": "2017-11-21T01:35:35.605477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.943562610229, log loss is 0.15680548436\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegressionCV()\n",
    "lr.fit(X_train_tfidf_rd, y_train)\n",
    "logls = (log_loss(y_test, lr.predict_proba(X_test_tfidf_rd), labels=[0, 1]))\n",
    "accuracy = sum(lr.predict(X_test_tfidf_rd) == y_test) * \\\n",
    "    1.0 / X_test_tfidf_rd.shape[0]\n",
    "\n",
    "print 'Accuracy is ' + str(accuracy) + ', log loss is ' + str(logls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. SVM\n",
    "\n",
    "SVM with linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T00:43:37.555709Z",
     "start_time": "2017-11-21T00:38:00.913323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9482657260435038, log loss is 0.169479085788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', probability=True)\n",
    "clf.fit(X_train_tfidf_rd, y_train)\n",
    "logls = (log_loss(y_test, clf.predict_proba(X_test_tfidf_rd), labels=[0, 1]))\n",
    "accuracy = sum(clf.predict(X_test_tfidf_rd) == y_test) * \\\n",
    "    1.0 / X_test_tfidf_rd.shape[0]\n",
    "\n",
    "print 'Accuracy is ' + str(accuracy) + ', log loss is ' + str(logls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. LSTM\n",
    "\n",
    "2-layer LSTM with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:37:48.033711Z",
     "start_time": "2017-11-21T01:37:30.134838Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import for LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from tensorflow.contrib import rnn\n",
    "from collections import Counter\n",
    "import tflearn\n",
    "import gensim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:38:04.689443Z",
     "start_time": "2017-11-21T01:38:04.267335Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./train.csv\")\n",
    "X_train = train_data[\"tweet\"]\n",
    "y_train = train_data.handle.map({\"HillaryClinton\": np.array(\n",
    "    [1.0, 0]), \"realDonaldTrump\": np.array([0, 1.0])})\n",
    "y_train = np.array(y_train.tolist(), dtype=float)\n",
    "\n",
    "test_data = pd.read_csv('./new_test.csv')\n",
    "X_test = test_data[\"tweet\"]\n",
    "y_test = test_data.handle.map({\"HillaryClinton\": np.array(\n",
    "    [1.0, 0]), \"realDonaldTrump\": np.array([0, 1.0])})\n",
    "y_test = np.array(y_test.tolist(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:38:09.350612Z",
     "start_time": "2017-11-21T01:38:09.344474Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(tknzr, tweet):\n",
    "    try:\n",
    "        tweet = tweet.lower()\n",
    "        tokens = tknzr.tokenize(tweet)\n",
    "        tokens = map(lambda t: t if not t.startswith(\n",
    "            'http') else '<url>', tokens)\n",
    "#         tokens = filter(lambda t: t not in stopwords.words('english'),tokens)\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:38:15.234695Z",
     "start_time": "2017-11-21T01:38:14.779266Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "train_count = 0\n",
    "X_train_tk = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    token_tmp = tokenize(tknzr, X_train[i])\n",
    "    tmp = []\n",
    "    for word in token_tmp:\n",
    "        if word[0] == '#' and word != '#':\n",
    "            tmp.append('<hashtag>')\n",
    "            tmp.append(word[1:].lower())\n",
    "        elif word[0] == '@' and word != '@':\n",
    "            tmp.append('<user>')\n",
    "            tmp.append(word[1:].lower())\n",
    "        elif word[0].isdigit():\n",
    "            tmp.append('<number>')\n",
    "        else:\n",
    "            tmp.append(word)\n",
    "\n",
    "    train_count = max(train_count, len(tmp))\n",
    "    X_train_tk.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:38:19.757156Z",
     "start_time": "2017-11-21T01:38:19.753233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You can go to https://t.co/tTgeqxNqYm to make sure you are registered. And I hope you all will.” —Hillary #NationalVoterRegistrationDay\n",
      "[u'\"', u'you', u'can', u'go', u'to', '<url>', u'to', u'make', u'sure', u'you', u'are', u'registered', u'.', u'and', u'i', u'hope', u'you', u'all', u'will', u'.', u'\\u201d', u'\\u2014', u'hillary', '<hashtag>', u'nationalvoterregistrationday']\n"
     ]
    }
   ],
   "source": [
    "print X_train[20]\n",
    "print X_train_tk[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:38:27.252308Z",
     "start_time": "2017-11-21T01:38:27.028725Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "test_count = 0\n",
    "X_test_tk = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    token_tmp = tokenize(tknzr, X_test[i])\n",
    "    tmp = []\n",
    "    for word in token_tmp:\n",
    "        if word[0] == '#' and word != '#':\n",
    "            tmp.append('<hashtag>')\n",
    "            tmp.append(word[1:].lower())\n",
    "        elif word[0] == '@' and word != '@':\n",
    "            tmp.append('<user>')\n",
    "            tmp.append(word[1:].lower())\n",
    "        elif word[0].isdigit():\n",
    "            tmp.append('<number>')\n",
    "        else:\n",
    "            tmp.append(word)\n",
    "\n",
    "    test_count = max(train_count, len(tmp))\n",
    "    X_test_tk.append(tmp)\n",
    "    \n",
    "X_test_tk = X_test_tk + X_test_tk[-66:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:38:30.737231Z",
     "start_time": "2017-11-21T01:38:30.077772Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    X_train_tk + X_test_tk, min_count=1, size=25, iter=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# model = {}\n",
    "# glove_data = 'data/glove.twitter.27B.25d.txt'\n",
    "# f = open(glove_data)\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     value = np.asarray(values[1:], dtype='float32')\n",
    "#     model[word] = value\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:39:19.592147Z",
     "start_time": "2017-11-21T01:39:19.246632Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "for word_list in X_train_tk:\n",
    "    vocab_list += word_list\n",
    "count = Counter(vocab_list)\n",
    "\n",
    "vocab_dict = dict()\n",
    "embedding_matrix = np.empty((0, 25), float)\n",
    "\n",
    "for word in count:\n",
    "    vocab_dict[word] = len(vocab_dict)\n",
    "    embedding_matrix = np.vstack((embedding_matrix, model[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:39:22.369546Z",
     "start_time": "2017-11-21T01:39:22.305453Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_vec = []\n",
    "X_train_length = []\n",
    "for i in range(len(X_train_tk)):\n",
    "    tmp = []\n",
    "    for word in X_train_tk[i]:\n",
    "        tmp.append(vocab_dict[word])\n",
    "    tmp = np.array(tmp)\n",
    "    X_train_vec.append(tmp)\n",
    "    X_train_length.append(len(tmp))\n",
    "X_train_vec = np.array(X_train_vec)\n",
    "X_train_length = np.array(X_train_length)\n",
    "\n",
    "X_test_vec = []\n",
    "X_test_length = []\n",
    "for i in range(len(X_test_tk)):\n",
    "    tmp = []\n",
    "    for word in X_test_tk[i]:\n",
    "        if word in vocab_dict:\n",
    "            tmp.append(vocab_dict[word])\n",
    "    tmp = np.array(tmp)\n",
    "    X_test_vec.append(tmp)\n",
    "    X_test_length.append(len(tmp))\n",
    "X_test_vec = np.array(X_test_vec)\n",
    "X_test_length = np.array(X_test_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:39:25.603208Z",
     "start_time": "2017-11-21T01:39:25.229563Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pad = tflearn.data_utils.pad_sequences(X_train_vec)\n",
    "X_test_pad = tflearn.data_utils.pad_sequences(\n",
    "    X_test_vec, maxlen=X_train_pad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:39:29.684986Z",
     "start_time": "2017-11-21T01:39:29.672198Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "training_steps = 20\n",
    "batch_size = 93\n",
    "display_step = 50\n",
    "embed_size = 25\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1\n",
    "time_step = X_train_pad.shape[1]\n",
    "num_hidden = 20\n",
    "num_classes = 2\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.int32, [None, time_step])\n",
    "X_length = tf.placeholder(tf.int32, [None])\n",
    "embedding = tf.Variable(embedding_matrix)\n",
    "Y = tf.placeholder(tf.float16, [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:39:33.732279Z",
     "start_time": "2017-11-21T01:39:33.717343Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:40:03.791070Z",
     "start_time": "2017-11-21T01:40:03.760658Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def RNN(x, x_length, weights, biases):\n",
    "\n",
    "    batch_size_tmp = tf.shape(x)[0]\n",
    "    embedding = tf.get_variable('embedding', [len(vocab_dict), embed_size])\n",
    "    embed = [tf.nn.embedding_lookup(embedding, row)\n",
    "             for row in tf.split(x, batch_size)]\n",
    "    embed = tf.reshape(embed, (batch_size_tmp, time_step, embed_size))\n",
    "    embed = tf.unstack(embed, time_step, 1)\n",
    "\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=0.5)\n",
    "    cell = rnn.MultiRNNCell([cell] * 1)\n",
    "\n",
    "    outputs, states = rnn.static_rnn(\n",
    "        cell, dtype=tf.float32, sequence_length=x_length, inputs=embed)\n",
    "\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    index = tf.range(0, batch_size_tmp) * \\\n",
    "        X_train_pad.shape[1] + tf.reshape(x_length - 1, [batch_size_tmp])\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, num_hidden]), index)\n",
    "\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:40:12.903514Z",
     "start_time": "2017-11-21T01:40:04.930436Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wjh/.local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "logits = RNN(X, X_length, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "tf.summary.histogram('logits', logits)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "tf.summary.scalar('loss', loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('./log/hw5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T01:41:58.578370Z",
     "start_time": "2017-11-21T01:40:51.606024Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 0.2469, Training Accuracy= 0.934\n",
      "Step 2, Minibatch Loss= 0.0680, Training Accuracy= 0.985\n",
      "Step 3, Minibatch Loss= 0.0272, Training Accuracy= 0.995\n",
      "Step 4, Minibatch Loss= 0.0169, Training Accuracy= 0.997\n",
      "Step 5, Minibatch Loss= 0.0102, Training Accuracy= 0.999\n",
      "Step 6, Minibatch Loss= 0.0065, Training Accuracy= 0.999\n",
      "Step 7, Minibatch Loss= 0.0049, Training Accuracy= 0.999\n",
      "Step 8, Minibatch Loss= 0.0147, Training Accuracy= 0.996\n",
      "Step 9, Minibatch Loss= 0.0061, Training Accuracy= 0.999\n",
      "Step 10, Minibatch Loss= 0.0038, Training Accuracy= 0.999\n",
      "Step 11, Minibatch Loss= 0.0028, Training Accuracy= 0.999\n",
      "Step 12, Minibatch Loss= 0.0024, Training Accuracy= 0.999\n",
      "Step 13, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n",
      "Step 14, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "Step 15, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "Step 16, Minibatch Loss= 0.0018, Training Accuracy= 0.999\n",
      "Step 17, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "Step 18, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Step 19, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Step 20, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "predict_all = []\n",
    "with tf.Session(config=config) as sess:\n",
    "    writer.add_graph(sess.graph)\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps + 1):\n",
    "        indexes = list(range(X_train_pad.shape[0]))\n",
    "        np.random.shuffle(indexes)\n",
    "        for i in range(1, X_train_pad.shape[0] // batch_size + 1):\n",
    "            batch_x, batch_y = X_train_pad[indexes[batch_size * (\n",
    "                i - 1):batch_size * i]], y_train[indexes[batch_size * (i - 1):batch_size * i]]\n",
    "            batch_x_length = X_train_length[indexes[batch_size *\n",
    "                                                    (i - 1):batch_size * i]]\n",
    "\n",
    "            batch_x_length = batch_x_length.reshape((-1))\n",
    "            summary, _ = sess.run([merged_summary, train_op], feed_dict={\n",
    "                                  X: batch_x, X_length: batch_x_length, Y: batch_y})\n",
    "            writer.add_summary(summary, step)\n",
    "\n",
    "        loss = []\n",
    "        acc = []\n",
    "        for i in range(1, X_train_pad.shape[0] // batch_size + 1):\n",
    "            batch_x, batch_y = X_train_pad[batch_size * (\n",
    "                i - 1):batch_size * i], y_train[batch_size * (i - 1):batch_size * i]\n",
    "            batch_x_length = X_train_length[batch_size *\n",
    "                                            (i - 1):batch_size * i]\n",
    "\n",
    "            batch_x_length = batch_x_length.reshape((-1))\n",
    "            loss_tmp, acc_tmp = sess.run([loss_op, accuracy], feed_dict={X: batch_x, X_length: batch_x_length,\n",
    "                                                                         Y: batch_y})\n",
    "            loss.append(loss_tmp)\n",
    "            acc.append(acc_tmp)\n",
    "\n",
    "        print(\"Step \" + str(step) + \", Minibatch Loss= \" +\n",
    "              \"{:.4f}\".format(np.mean(loss[:1701])) + \", Training Accuracy= \" +\n",
    "              \"{:.3f}\".format(np.mean(acc[:1701])))\n",
    "\n",
    "        predict_all = np.empty((0, 2), float)\n",
    "        for i in range(1, X_test_pad.shape[0] // batch_size + 1):\n",
    "            batch_x, batch_y = X_test_pad[batch_size * (\n",
    "                i - 1):batch_size * i], y_test[batch_size * (i - 1):batch_size * i]\n",
    "            batch_x_length = X_test_length[batch_size * (i - 1):batch_size * i]\n",
    "\n",
    "            batch_x_length = batch_x_length.reshape((-1))\n",
    "            predict = sess.run(prediction, feed_dict={\n",
    "                               X: batch_x, X_length: batch_x_length, Y: batch_y})\n",
    "            predict_all = np.vstack((predict_all, np.array(predict)))\n",
    "\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with open('res.csv', 'w') as f:\n",
    "    f.write('id,realDonaldTrump,HillaryClinton\\n')\n",
    "    for i in range(1701):\n",
    "        f.write('{},{:0.6f},{:0.6f}\\n'.format(\n",
    "            i, predict_all[i, 1], predict_all[i, 0]))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
